{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from bert_embedding import BertEmbedding\n",
    "\n",
    "# bert_abstract = \"\"\"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.\n",
    "#  Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers.\n",
    "#  As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. \n",
    "# BERT is conceptually simple and empirically powerful. \n",
    "# It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7 (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5% absolute improvement), outperforming human performance by 2.0%.\"\"\"\n",
    "\n",
    "# sentences = bert_abstract.split('\\n')\n",
    "# bert_embedding = BertEmbedding()\n",
    "# result = bert_embedding(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulberard/opt/anaconda3/envs/projet-anlp/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n",
      "100%|██████████| 995526/995526 [00:00<00:00, 2008497.95B/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary-multilingual)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'I', 'am', 'so', 'happy', ',', 'today', 'is', 'my', 'birthday', '!', 'I', 'can', \"'\", 't', 'wait', 'to', 'dance', 'with', 'my', 'friends', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"I am so happy, today is my birthday! I can't wait to dance with my friends!\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 662804195/662804195 [01:53<00:00, 5847237.62B/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "#model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23, 12, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "\n",
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# `encoded_layers` has shape [12 x 1 x 23 x 768]\n",
    "\n",
    "# `token_vecs` is a tensor with shape [23 x 768]\n",
    "token_vecs = encoded_layers[11][0]\n",
    "\n",
    "# Calculate the average of all 23 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final sentence embedding vector of shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our final sentence embedding vector of shape:\", sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.1597e-02, -4.1800e-01, -1.6561e-02,  1.7259e-01,  6.4464e-01,\n",
       "         2.2431e-01,  3.5428e-03, -2.0353e-01, -1.7712e-01,  1.9254e-01,\n",
       "         6.3306e-02,  4.9740e-01,  2.8774e-01,  1.3299e-01,  4.8750e-01,\n",
       "        -2.4759e-01,  6.0300e-01, -3.3866e-01,  3.6454e-02, -9.3834e-02,\n",
       "         4.5473e-02,  4.5309e-02,  7.9374e-02,  2.0791e-03,  1.2832e-01,\n",
       "         6.7566e-01, -3.5628e-01, -3.9083e-03, -1.5017e-01, -5.3016e-01,\n",
       "         3.3515e-02,  5.1584e-01, -3.1449e-01,  8.3520e-01, -1.4942e-02,\n",
       "         3.1487e-01, -3.4166e-02,  4.8147e-02,  2.3929e-01, -6.6266e-02,\n",
       "        -4.6305e-02,  2.4869e-01,  1.9579e-01, -2.0513e-01,  1.8091e-02,\n",
       "        -3.7220e-01,  4.7565e-01,  4.8067e-01, -2.3334e-02, -1.6669e-01,\n",
       "         1.3784e-01, -1.6271e-01,  5.7715e-01,  2.7957e-01,  1.1482e-01,\n",
       "         4.0967e-01, -1.1780e-02,  2.2493e-02,  3.6824e-01, -1.2241e-01,\n",
       "         1.1530e-01,  1.1278e-01,  4.0873e-01, -5.8943e-01, -3.8034e-01,\n",
       "         2.0094e-01, -1.5384e-02, -6.2539e-03,  1.0632e-01,  3.1196e-01,\n",
       "        -9.3767e-02,  3.3057e-01,  2.1921e-01,  3.5482e-01, -1.6013e-01,\n",
       "         8.7128e-01, -2.1817e-02, -3.8375e-01, -1.4289e-01, -4.2116e-01,\n",
       "         4.2974e-01,  4.4269e-01,  2.1735e-01,  2.5068e-01,  2.3162e-01,\n",
       "        -1.5073e-02,  5.8037e-02, -4.2911e-02,  7.3584e-01, -6.5640e-01,\n",
       "         4.0555e-01,  1.7418e-01,  9.4870e-01,  6.9994e-01, -1.8760e-01,\n",
       "         6.8141e-01,  1.1010e-01,  1.1987e-01,  1.9974e-01,  7.3091e-02,\n",
       "        -3.3375e-01, -1.7855e-02,  6.6529e-01, -3.0289e-01,  4.7738e-01,\n",
       "        -1.9008e-01,  1.0842e-01,  6.5237e-03,  2.9955e-01,  4.0934e-01,\n",
       "         3.9495e-01,  1.5076e-01, -9.6449e-02, -1.3961e-01,  1.5823e-01,\n",
       "        -2.2652e-01, -1.8566e-02,  2.3639e-01,  3.3151e-01, -1.2244e-01,\n",
       "         9.9053e-02, -1.6980e-01, -4.1496e-01,  5.0921e-01,  5.5980e-02,\n",
       "        -1.0042e-01, -1.9854e-01, -6.2722e-02, -3.5523e-01, -2.0463e-01,\n",
       "        -3.1262e-01, -1.6220e-01,  4.1495e-02,  1.5275e-01,  2.9358e-01,\n",
       "         1.6672e-01,  3.2262e-02,  2.2249e-01,  2.9636e-01,  4.8251e-01,\n",
       "        -6.1725e-02, -2.2598e-01, -2.6434e-01, -1.1574e-02, -3.3809e-01,\n",
       "        -2.5310e-01, -5.1033e-01, -1.1215e-01,  1.1783e-01, -3.7549e-01,\n",
       "        -4.4358e-01,  3.6189e-01,  3.5332e-01,  2.0785e-02, -2.2200e-01,\n",
       "         2.8843e-01,  4.4050e-01,  1.4803e-01, -1.7162e-01,  4.4371e-01,\n",
       "         1.3009e-01,  2.4779e-01, -6.4669e-01, -7.6679e-03, -2.7012e-01,\n",
       "        -1.9341e-01,  9.0570e-02, -1.9791e-01, -3.2101e-01,  4.8518e-01,\n",
       "        -2.7643e-01,  2.1162e-01, -5.2590e-01, -1.4324e-01,  6.3145e-01,\n",
       "         6.3909e-02, -1.9911e-01, -2.2745e-01, -6.3236e-02, -4.0144e-01,\n",
       "        -3.9667e-01,  6.1989e-01,  1.2777e-01, -6.6901e-01,  1.9347e-01,\n",
       "         1.4865e-01,  2.9096e-02, -5.8946e-02, -3.5463e-02, -8.2178e-02,\n",
       "        -1.4703e-01, -1.9143e-01, -1.5695e-01,  1.9830e-01, -1.5879e-01,\n",
       "         5.0223e-01, -1.3481e-01,  2.9846e-01,  2.0830e-01,  3.1832e-01,\n",
       "        -5.3975e-01, -2.7023e-01,  1.6519e-01, -7.8582e-02,  1.2786e-03,\n",
       "         2.0371e-01, -3.6245e-01, -1.1815e-01, -1.5432e-01,  8.6446e-01,\n",
       "        -2.5269e-02, -8.1965e-02,  3.4958e-01,  2.4695e-01,  5.1400e-01,\n",
       "        -3.4627e-02,  5.0649e-02,  2.7391e-02,  7.0892e-02, -8.3622e-02,\n",
       "         2.2455e-01,  2.6326e-02, -1.9623e-01, -5.5888e-03,  4.5494e-02,\n",
       "        -3.0536e-01,  1.0251e-01,  2.4228e-01, -1.6456e-01, -6.5903e-01,\n",
       "        -2.4044e-01, -6.1217e-01, -5.8040e-01,  2.6132e-01, -1.6502e-01,\n",
       "         1.5247e-01,  2.1433e-01, -1.5907e-01, -1.5220e-01,  1.5455e-02,\n",
       "        -7.6921e-02,  1.7899e-01, -1.3768e-01, -1.5626e-01,  2.2879e-01,\n",
       "         2.2295e-01, -2.8077e-01, -7.3048e-01,  7.6686e-02, -6.9101e-02,\n",
       "        -1.5023e-01,  9.9438e-02, -2.1711e-01, -2.4363e-02, -3.1373e-01,\n",
       "         2.3405e-01, -3.9325e-01,  1.3950e-01,  1.3081e-01, -2.8000e-01,\n",
       "         1.7772e-01, -1.0739e+00, -2.5218e-01, -2.0552e-01,  2.1562e-01,\n",
       "         3.2118e-02,  1.9750e-01, -8.7762e-02, -1.1233e-01,  1.1452e-01,\n",
       "        -4.8891e-01,  6.6811e-01,  4.1900e-01,  1.8453e-01,  1.4101e-01,\n",
       "         1.3125e-01,  1.8776e-01, -1.0201e-01, -6.3190e-01, -1.6284e-01,\n",
       "        -5.3147e-01, -4.4089e-01, -3.6002e-02, -2.5765e-01, -3.6274e-01,\n",
       "        -1.2668e-01, -1.0223e-01, -1.4496e-01, -5.8871e-04, -6.8100e-01,\n",
       "        -5.4068e-01, -1.1686e-01, -1.3458e-01,  3.5921e-01, -3.9850e-01,\n",
       "        -4.2003e-01, -9.6884e-01,  1.1042e-01, -1.3662e-01, -1.7085e-01,\n",
       "         4.2803e-01,  3.9616e-01, -3.2956e-02, -1.6014e-01, -4.1762e-01,\n",
       "         7.4045e-02, -4.4659e-02, -4.1079e-01,  8.6788e-01,  1.0858e-02,\n",
       "        -5.0983e-01,  5.3288e-01, -3.1443e-01,  4.9202e-01,  1.8316e-01,\n",
       "        -2.6382e-01,  2.4589e-01,  2.1652e-02, -3.0930e-01,  1.8646e-01,\n",
       "        -2.1042e-01, -4.9564e-02,  2.7939e-01, -2.1212e-01, -2.5539e-01,\n",
       "        -6.5954e-03,  7.2119e-02, -4.6799e-01,  1.1180e-01,  8.1349e-02,\n",
       "         1.5615e-01,  1.5922e+00,  2.1207e-01, -6.9400e-01, -3.5968e-01,\n",
       "         2.1105e-01, -2.0529e-02, -2.3763e-01, -4.7682e-01, -2.1244e-01,\n",
       "        -2.4414e-01,  3.9579e-02,  7.0589e-01, -1.0487e-01, -1.8404e-02,\n",
       "        -2.1156e-01, -5.6187e-02,  5.9724e-01, -1.2117e-01, -6.5906e-01,\n",
       "         1.0762e-01, -1.0844e-01,  2.8023e-01,  2.5138e-01,  3.3756e-01,\n",
       "         7.4843e-02, -6.1099e-01,  2.2080e-01,  2.2844e-01, -4.7190e-02,\n",
       "        -1.4597e-01,  2.3613e-01, -9.9865e-02, -1.4351e-01, -5.6012e-01,\n",
       "         6.4031e-01, -2.3757e-01,  2.9986e-01,  3.7311e-01,  8.2891e-01,\n",
       "         4.6387e-01,  1.2681e+00, -6.6627e-01, -7.1936e-01,  2.9691e-01,\n",
       "         3.2844e-01, -9.0719e-02, -7.4375e-01,  2.8588e-01,  9.8503e-02,\n",
       "         5.2097e-01, -3.3907e-01, -2.5912e-02, -1.0876e-01,  2.9547e-01,\n",
       "        -1.1053e-01,  3.9201e-01, -2.0178e-01,  5.2792e-01, -3.8134e-01,\n",
       "        -2.5315e-02,  3.9475e-01, -1.2902e-01,  1.0294e-01,  2.4571e-01,\n",
       "        -1.7012e-01,  3.8166e-01, -7.6206e-02, -6.0559e-01, -1.9012e-01,\n",
       "         2.7528e-01, -2.2678e-01,  3.3935e-01,  3.4801e-01,  2.0629e-01,\n",
       "         4.6817e-01,  3.1919e-01, -1.2837e-01,  1.0010e-01,  1.6931e-01,\n",
       "         8.6383e-04,  3.0814e-01,  4.8174e-01,  2.4995e-01,  4.3677e-01,\n",
       "         5.2059e-01, -7.2924e-01,  5.3927e-02,  2.7092e-01, -9.8542e-02,\n",
       "         5.6327e-02, -8.5313e-02,  2.0340e-01, -1.3354e+00, -2.4534e-01,\n",
       "         1.1198e-01, -1.8353e-01,  6.6634e-02, -7.3200e-01, -4.1698e-01,\n",
       "        -2.2518e-01, -2.9775e-02,  1.7694e-01, -2.2059e-01, -3.8267e-01,\n",
       "        -1.8701e-01, -8.3643e-03,  5.1735e-02, -1.0337e-01,  2.9232e-02,\n",
       "         4.1676e-03, -3.4919e-02, -2.5369e-01, -4.9062e-02,  1.6832e-01,\n",
       "         3.5129e-01,  4.5074e-02, -1.9547e-01,  1.0681e-01,  3.6621e-01,\n",
       "         6.7616e-01, -6.9741e-01, -1.4624e-01, -2.1003e-01, -1.6612e-02,\n",
       "        -1.3525e-01, -4.4824e-01, -2.0067e-02,  6.1411e-03, -7.7032e-01,\n",
       "        -2.4425e-01,  2.8163e-01,  7.8732e-02,  1.3223e-01, -1.0302e-01,\n",
       "        -3.3577e-01, -1.2588e-01, -7.3832e-02,  2.1368e-01,  2.5144e-01,\n",
       "        -4.3265e-01,  1.4579e-01, -3.1770e-01, -2.3531e-02, -2.6895e-01,\n",
       "         4.0692e-02,  3.6953e-01, -4.6882e-01,  8.9234e-03, -1.5871e-01,\n",
       "         9.3332e-02,  3.6480e-01,  5.4393e-01, -1.2062e-01,  2.7820e-01,\n",
       "        -9.3292e-04,  2.6924e-01, -1.1489e-01,  2.8057e-01,  3.7597e-02,\n",
       "        -8.7749e-02,  9.6057e-02, -3.8767e-01, -1.3152e+00,  6.4085e-02,\n",
       "        -6.7353e-05, -1.1504e-01, -4.7786e-01,  2.3728e-01,  3.2902e-01,\n",
       "        -2.3987e-01,  1.0072e-01, -1.8015e-01, -1.1905e-01, -1.8981e-01,\n",
       "        -2.6272e-01, -2.6980e-01,  1.3688e-01, -1.7625e-01,  3.1775e-01,\n",
       "         8.3034e-02, -7.2978e-01, -2.1898e-01,  4.6609e-01, -1.4088e-01,\n",
       "         2.4745e-01,  3.3642e-01,  8.0856e-02,  2.0896e-02, -3.8818e-01,\n",
       "         2.6249e-01,  7.7931e-03,  6.2726e-01, -2.9242e-02,  2.5846e-01,\n",
       "        -1.5636e-01, -2.1077e-01, -3.7418e-01, -1.5127e-02,  2.2152e-01,\n",
       "        -1.0717e-01, -6.3282e-02,  3.6456e-02,  1.4350e-01, -5.8271e-02,\n",
       "        -4.2290e-03,  2.0307e-01,  3.0754e-01,  9.8818e-02,  2.7626e-01,\n",
       "        -8.1485e-02,  1.6178e-02, -6.2484e-02,  2.5532e-01,  1.3101e-01,\n",
       "        -8.0165e-03,  5.9601e-02,  1.1383e-01, -5.2272e-01, -1.8248e-01,\n",
       "         7.1864e-02, -2.6524e-01,  5.8628e-02,  1.6025e-01, -2.1378e-02,\n",
       "         4.3823e-01,  2.1072e-01, -2.5470e-01,  3.6958e-01,  2.1876e-02,\n",
       "        -3.8785e-02,  8.2586e-02,  4.5278e-01, -3.8400e-02,  3.3486e-01,\n",
       "        -2.2038e-01, -6.4497e-01,  5.8640e-02, -2.3769e-01, -1.2780e-01,\n",
       "         2.8987e-01,  1.7912e-01, -1.1377e-01, -2.3961e-02, -9.2010e-02,\n",
       "        -2.1277e-02,  8.4987e-02,  3.4188e-03, -1.9363e-02, -2.6790e-01,\n",
       "        -3.2455e-01, -2.2096e-01,  5.6495e-02,  1.2142e-01, -3.1911e-01,\n",
       "        -4.1926e-01, -2.6157e-01,  4.9103e-01,  1.5174e-01, -3.9076e-01,\n",
       "         2.1760e-01,  3.9228e-01,  6.3396e-02, -9.0034e-02,  2.2536e-01,\n",
       "        -5.5917e-01, -1.8975e-01, -1.3702e-01,  1.8946e-01,  4.3482e-02,\n",
       "         2.8685e-02, -5.6467e-01, -3.3481e-01, -3.4569e-01, -7.8025e-01,\n",
       "        -1.9272e-01,  2.3114e-01, -8.1350e-02, -4.0201e-01, -2.5824e-01,\n",
       "        -7.6373e-02,  1.0375e-01,  9.9100e-02, -4.2235e-01, -2.5448e-01,\n",
       "        -7.9717e-01, -3.2907e-01,  9.7071e-02,  1.8065e-01,  3.1131e-01,\n",
       "         3.5692e-01, -2.4199e-01,  4.2364e-01,  7.0495e-02,  4.6309e-01,\n",
       "         4.4737e-01,  2.9729e-01,  1.3459e-01,  1.9492e-01,  6.8523e-02,\n",
       "        -2.9098e-01, -3.9174e-01, -3.4447e-01,  3.0680e-01, -4.0380e-02,\n",
       "         1.5338e-01,  1.2035e-01,  9.9833e-02,  9.0377e-03,  2.9920e-01,\n",
       "        -3.0475e-01,  3.0830e-01,  1.5265e-01,  6.6269e-02, -5.9450e-02,\n",
       "         1.6732e-01, -4.3785e-02,  5.3559e-01, -4.4423e-01, -2.7401e-01,\n",
       "         7.2915e-01, -4.1416e-01,  3.8472e-01,  1.5226e-01,  1.0644e-01,\n",
       "         3.9759e-01, -1.0292e-01,  2.1691e-01, -6.2042e-01,  6.5299e-02,\n",
       "         4.1464e-01,  5.1984e-01,  9.2472e-02, -7.0495e-01, -7.4241e-02,\n",
       "         4.5967e-02, -4.7976e-01, -7.3059e-02,  2.1783e-01,  7.7131e-02,\n",
       "        -2.0417e-01,  2.6155e-01,  2.2833e-01,  1.5690e-01,  2.9762e-01,\n",
       "         1.4494e-01,  2.0477e-01, -1.4023e-03, -7.2261e-02, -5.4614e-02,\n",
       "        -5.9857e-01,  4.5101e-02, -1.2240e-01, -8.6696e-02,  1.3590e-01,\n",
       "        -8.3096e-02, -4.2374e-03,  2.4368e-01,  8.9669e-02,  5.0862e-02,\n",
       "         1.0075e-01,  1.8666e-01,  1.8392e-01, -2.6492e-01, -4.5522e-01,\n",
       "         6.7190e-04,  1.7728e-01, -3.5498e-01,  5.4304e-01, -2.2986e-01,\n",
       "        -7.6389e-02, -8.1994e-01, -7.0940e-02, -2.8491e-01, -2.4364e-01,\n",
       "        -5.2521e-01, -3.9843e-01, -3.3545e-01, -1.7584e-01, -8.5143e-03,\n",
       "         5.0319e-01, -2.2555e-01,  2.4980e-01, -2.1149e-01, -1.9841e-01,\n",
       "        -2.8985e-01,  1.7531e-01, -5.8987e-01, -3.7205e-01,  3.8539e-01,\n",
       "        -3.4633e-02,  2.3268e-01, -1.1774e-01, -4.7289e-02,  2.3869e-01,\n",
       "         4.6659e-02,  1.4439e-01,  2.1781e-01, -3.1061e-02, -7.2107e-02,\n",
       "         1.8631e-01, -2.0756e+00,  2.5164e-02, -8.6761e-02,  2.1516e-01,\n",
       "        -4.7991e-01,  2.3149e-01,  7.1010e-01, -2.6272e-01,  6.6106e-02,\n",
       "         1.2274e-01, -2.3107e-01, -2.9399e-01, -1.8589e-01, -2.8491e-02,\n",
       "         1.8089e-02,  6.1233e-01,  4.8657e-01,  9.6788e-01, -4.5476e-01,\n",
       "        -2.2069e-01,  2.6507e-01, -2.9539e-01, -4.5743e-01,  3.9917e-02,\n",
       "        -1.2492e-01, -3.8522e-01,  3.9380e-01,  3.6377e-01,  6.8660e-02,\n",
       "        -4.8267e-01, -2.6426e-01, -4.8098e-01, -8.3566e-01,  3.6342e-01,\n",
       "         9.5399e-02, -4.8718e-01, -1.3019e-01])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
